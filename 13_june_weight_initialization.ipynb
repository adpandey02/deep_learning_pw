{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9198b-3063-494f-8d35-31710ba6e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Part 1: Understanding Weight Initialization\n",
    "Weight initialization is a crucial step in artificial neural networks as it sets the initial values of the weights connecting the neurons. Proper initialization is necessary to ensure effective learning and convergence during the training process. Here are some reasons why careful weight initialization is important:\n",
    "Breaking Symmetry: In neural networks, all neurons in a layer receive the same gradients during backpropagation. If the weights are initialized to the same value, all neurons will update their weights in the same way, leading to symmetry among neurons. This symmetry can hinder the learning process, as the network will be unable to learn diverse and useful representations. By carefully initializing weights, we can break this symmetry and encourage the network to learn unique features.\n",
    "Avoiding Vanishing/Exploding Gradients: Improper weight initialization can lead to the problem of vanishing or exploding gradients. Vanishing gradients occur when the gradients become very small, making it difficult for the network to update the weights effectively. On the other hand, exploding gradients happen when the gradients become very large, causing instability and convergence issues. Proper weight initialization techniques help mitigate these problems and enable more stable gradient flows.\n",
    "Challenges associated with improper weight initialization can significantly impact model training and convergence:\n",
    "Slow Convergence: If the weights are not initialized properly, the network may converge slowly or struggle to converge at all. This can result in longer training times and increased computational costs.\n",
    "Unstable Training: Improper initialization can lead to unstable training dynamics. The network may exhibit erratic behavior, such as oscillating or diverging loss values, making it challenging to achieve the desired performance.\n",
    "Poor Generalization: Inadequate weight initialization can cause the network to get stuck in suboptimal solutions or fail to generalize well to unseen data. This can result in reduced model performance on both the training set and test set.\n",
    "\n",
    "Variance is a statistical term that measures the spread or dispersion of a set of values. In the context of weight initialization, variance\n",
    "refers to the spread of the initial weight values. It is crucial to consider the variance during initialization for the following reasons:\n",
    "Activation Saturation: If the variance is too high, the activations in the network can become saturated. Saturation occurs when the input\n",
    "to an activation function is either very large or very small, causing the gradient to be close to zero. This hampers the learning process\n",
    "as the weights are not updated effectively.\n",
    "\n",
    "Gradient Scaling: Variance affects the scale of the gradients during backpropagation. If the variance is too large, it can result\n",
    "in large gradients, leading to unstable training. Conversely, if the variance is too small, the gradients may become too small, causing\n",
    "slow convergence or vanishing gradients.\n",
    "\n",
    "\n",
    "Part 2: Weight Initialization Techniques\n",
    "a. Xavier/Glorot Initialization: Xavier initialization aims to set the initial weights such that the variance of the activations remains\n",
    "constant across layers. It achieves this by scaling the random initial weights based on the number ofinputs to the neuron and the number \n",
    "of neurons in the previous layer. The underlying theory is that if the weights are too small, the signal will diminish as it propagates \n",
    "through the network, and if the weights are too large, the signal will explode. By carefully initializing the weights with appropriate \n",
    "scaling, Xavier initialization helps alleviate these issues, leading to more stable training and convergence.\n",
    "b. He Initialization: He initialization, also known as the He et al. initialization, is an extension of Xavier initialization. It takes\n",
    "into account the rectified linear unit (ReLU) activation function, which is commonly used in deep neural networks. He initialization \n",
    "scales the random initial weights based on the number of inputs to the neuron, similar to Xavier initialization. However, He \n",
    "initialization uses a different scaling factor that accounts for the specific properties of ReLU activation. He initialization is \n",
    "preferred over Xavier initialization when using ReLU or its variants as activation functions, as it provides better initialization \n",
    "for the rectifying non-linearity of ReLU.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f4b68-4569-4a14-8255-ca97ab0f4186",
   "metadata": {},
   "source": [
    "### part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c949af-c5da-4dfd-a681-6f71e179f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 20:33:42.368710: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-05 20:33:42.818452: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-05 20:33:42.820568: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-05 20:33:44.399161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Xavier Initialization: 0.9629999995231628\n",
      "Accuracy for He Initialization: 0.9624999761581421\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(25, activation=\"relu\", kernel_initializer=\"glorot_uniform\", name=\"xavier_init\"),\n",
    "    layers.Dense(25, activation=\"relu\", kernel_initializer=\"he_uniform\", name=\"he_init\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the models with different weight initialization techniques\n",
    "history_xavier_init = model.get_layer(\"xavier_init\").set_weights(model.get_layer(\"xavier_init\").get_weights())\n",
    "history_he_init = model.get_layer(\"he_init\").set_weights(model.get_layer(\"he_init\").get_weights())\n",
    "\n",
    "history_xavier_init = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=64, epochs=10, verbose=0)\n",
    "history_he_init = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=64, epochs=10, verbose=0)\n",
    "\n",
    "# Compare the performance of the initialized models\n",
    "accuracy_xavier_init = history_xavier_init.history[\"val_accuracy\"]\n",
    "accuracy_he_init = history_he_init.history[\"val_accuracy\"]\n",
    "\n",
    "print(\"Accuracy for Xavier Initialization:\", accuracy_xavier_init[-1])\n",
    "print(\"Accuracy for He Initialization:\", accuracy_he_init[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1ace4-9bd5-47c2-88c5-e46130b366e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the choice of weight initialization technique should be based on a careful analysis of the specific characteristics of your \n",
    "neural network architecture, the activation functions used, the complexity of the task, and the available data. It may require \n",
    "experimentation and iterative refinement to find the most effective weight initialization technique for your particular scenario.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
