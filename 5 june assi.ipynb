{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb708de-7764-4f49-8d3b-002d2cf4b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. An activation function in the context of artificial neural networks is a mathematical function that is applied to the \n",
    "output of each neuron in a neural network. It introduces non-linearity to the network, allowing it to learn and model complex\n",
    "relationships between inputs and outputs.\n",
    "\n",
    "Q2. Some common types of activation functions used in neural networks are:\n",
    "Sigmoid Function: It maps the input to a value between 0 and 1, representing the probability of the neuron being activated.\n",
    "Rectified Linear Unit (ReLU): It returns the input if it is positive, otherwise, it returns zero.\n",
    "Hyperbolic Tangent (tanh): It maps the input to a value between -1 and 1, providing a stronger gradient than the sigmoid function.\n",
    "Softmax Function: It normalizes the outputs of a neural network into a probability distribution over multiple classes.\n",
    "\n",
    "Q3. Activation functions affect the training process and performance of a neural network in several ways. They introduce \n",
    "non-linearity, which allows the network to learn complex patterns and make predictions on a wide range of data. Activation functions \n",
    "also influence the convergence speed during training and the ability of the network to handle gradient propagation.\n",
    "\n",
    "Q4. The sigmoid activation function, also known as the logistic function, maps the input to a value between 0 and 1. It is defined \n",
    "as f(x) = 1 / (1 + exp(-x)). The sigmoid function squashes the input into a probability-like output, which can be useful in binary\n",
    "classification problems. However, it has some disadvantages such as the vanishing gradient problem, where gradients become very small\n",
    "for inputs with large magnitudes, making it challenging for deep networks to train effectively. Sigmoid functions also suffer from \n",
    "the \"saturation\" problem, where the output becomes insensitive to small changes in the input when it approaches the extremes (0 or 1).\n",
    "\n",
    "Q5. The rectified linear unit (ReLU) activation function is defined as f(x) = max(0, x). It returns the input directly if it\n",
    "is positive, otherwise, it outputs zero. ReLU is a popular activation function due to its simplicity and computational efficiency. It\n",
    "overcomes some of the limitations of the sigmoid function, such as the vanishing gradient problem, and it allows the network to learn\n",
    "faster during training. Unlike the sigmoid function, ReLU does not saturate for positive inputs, which helps to alleviate the gradient\n",
    "vanishing issue.\n",
    "\n",
    "Q6. The benefits of using the ReLU activation function over the sigmoid function include:\n",
    "Computationally efficient: ReLU is simpler to compute compared to the sigmoid function and its derivatives.\n",
    "Avoids vanishing gradients: ReLU helps mitigate the vanishing gradient problem since it does not saturate for positive inputs.\n",
    "Faster convergence: ReLU allows for faster training convergence due to its ability to provide a stronger gradient signal during backpropagation.\n",
    "Sparse activation: ReLU can lead to sparsity in the network by setting negative values to zero, which can improve efficiency and generalization.\n",
    "\n",
    "Q7. The \"leaky ReLU\" is a variant of the ReLU activation function that addresses the vanishing gradient problem. In the leaky ReLU, instead \n",
    "of setting negative values to zero, a small negative slope is assigned to negative inputs. Mathematically, it is defined as f(x) = max(ax, x)\n",
    "where a is a small constant (e.g., 0.01). By introducing a small gradient for negative inputs, leaky ReLU allows for the propagation of\n",
    "gradients even when the neuron is not activated. This helps prevent the complete \"death\" of neurons and enables better learning in deep networks.\n",
    "\n",
    "Q8. The softmax activation function is commonly used in the output layer of a neural network for multi-class classification problems. It \n",
    "converts the outputs of the previous layer into a probability distribution over the classes. The softmax function normalizes the outputs\n",
    "ensuring that the sum of the probabilities for all classes is equal to 1. It allows the network to provide relative probabilities \n",
    "for each class, aiding in decision-making and selecting the most likely class.\n",
    "\n",
    "Q9. The hyperbolic tangent (tanh) activation function is defined as f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)). It maps the input\n",
    "to a value between -1 and 1, similar to the sigmoid function. Compared to the sigmoid function, tanh provides a stronger gradient, which\n",
    "can help with faster convergence during training. However, like the sigmoid function, tanh can also suffer from the vanishing gradient\n",
    "problem. Additionally, tanh is zero-centered, which can be advantageous in certain cases where zero-centered data is desired.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
